{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7d9de4-75fc-45cf-9b6a-592745a76df0",
   "metadata": {},
   "source": [
    "# 3. Modeling & Final Training\n",
    "\n",
    "_In this notebook we:_  \n",
    "1. Load the fully processed dataset from `data/processed/historical_proc.csv`.  \n",
    "2. Verify that the target column (`target_up`) exists.  \n",
    "3. Impute any remaining missing values.  \n",
    "4. Define and tune our classifiers and regressors.  \n",
    "5. Use TimeSeriesSplit for honest CV.  \n",
    "6. Select the best classifier model.  \n",
    "7. Retrain that classifier on 100% of the data.  \n",
    "8. Save the final model for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a92773",
   "metadata": {},
   "source": [
    "# 1. Parameters for Papermill & runtime  \n",
    "These control where we read/write and whether to force GPU usage.  \n",
    "- `DATA_PROCESSED`: path to processed data  \n",
    "- `MODEL_DIR`: where to save final models  \n",
    "- `USE_GPU`: toggle GPU‐based training (CI must set this to `False`)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a05e415",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters cell for Papermill\n",
    "DATA_PROCESSED = \"data/processed\"\n",
    "MODEL_DIR = \"src/models\"\n",
    "USE_GPU = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c8792b-b3bd-47bf-b551-33aa5c82b407",
   "metadata": {},
   "source": [
    "### 2. Imports & Warnings Configuration\n",
    "Load modeling libraries, metrics, and GPU‐enabled boosters; suppress known warnings for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0479201-b730-4db0-90eb-adbc74de367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 – Imports & GPU flag\n",
    "\n",
    "# 1) Use fork start method to avoid multiprocessing.resource_tracker errors\n",
    "import multiprocessing as mp\n",
    "mp.set_start_method('fork', force=True)\n",
    "\n",
    "import os, warnings\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV, GridSearchCV, TimeSeriesSplit,\n",
    "    ParameterGrid, cross_val_score\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, StackingClassifier,\n",
    "    RandomForestRegressor\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    mean_squared_error, precision_recall_curve\n",
    ")\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "import joblib\n",
    "\n",
    "# Suppress deprecation & device warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*not used.*\")\n",
    "\n",
    "# Matplotlib & Seaborn setup\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "\n",
    "# Autotime for per-cell timing\n",
    "%load_ext autotime\n",
    "\n",
    "# Determine at runtime whether to use GPU\n",
    "USE_GPU = os.environ.get(\"CI_USE_GPU\", \"false\").lower() in (\"1\",\"true\",\"yes\")\n",
    "print(f\"→ USE_GPU = {USE_GPU}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b9a3eb-3f09-4c99-a1c6-c9caba8a7322",
   "metadata": {},
   "source": [
    "### 3. Load Features & Build Targets\n",
    "1. Debug‐print contents of `DATA_PROCESSED`.\n",
    "2. Read `historical_proc.csv`, sort chronologically.\n",
    "3. Define next‐day targets: `target_up` (binary), `target_high`, `target_low`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d7a1c-eeae-4d6b-a735-435387e3a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DEBUG: data/processed contains:\", os.listdir(DATA_PROCESSED))\n",
    "\n",
    "df = pd.read_csv(os.path.join(DATA_PROCESSED, \"historical_proc.csv\"),\n",
    "                 parse_dates=[\"date\"])\n",
    "df.sort_values(\"date\", inplace=True)\n",
    "\n",
    "# create next‐day columns\n",
    "df[\"close_next\"] = df[\"close\"].shift(-1)\n",
    "df[\"high_next\"]  = df[\"high\"].shift(-1)\n",
    "df[\"low_next\"]   = df[\"low\"].shift(-1)\n",
    "\n",
    "# define targets\n",
    "df[\"target_up\"]   = (df[\"close_next\"] > df[\"close\"]).astype(int)\n",
    "df[\"target_high\"] = df[\"high_next\"]\n",
    "df[\"target_low\"]  = df[\"low_next\"]\n",
    "\n",
    "# drop final row with NaN targets\n",
    "df.dropna(subset=[\"target_up\",\"target_high\",\"target_low\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43dbf95-228a-4354-a4f9-62aecc469d80",
   "metadata": {},
   "source": [
    "### 4. Train/Test Split\n",
    "Select only feature columns, then split first 75 % of rows for training and last 25 % for testing (time‐aware)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab42e11-52fe-4565-bfac-20e339fcb1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [c for c in df.columns\n",
    "                if c.endswith((\"_std\",\"_mm\")) or c.startswith(\"PC\") or c in [\"year\",\"month\",\"day\",\"weekday\"]]\n",
    "\n",
    "X     = df[feature_cols]\n",
    "y_up  = df[\"target_up\"]\n",
    "y_high = df[\"target_high\"]\n",
    "y_low  = df[\"target_low\"]\n",
    "\n",
    "split = int(len(df) * 0.75)\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_up_train, y_up_test   = y_up.iloc[:split], y_up.iloc[split:]\n",
    "y_high_train, y_high_test = y_high.iloc[:split], y_high.iloc[split:]\n",
    "y_low_train,  y_low_test  = y_low.iloc[:split],  y_low.iloc[split:]\n",
    "\n",
    "print(f\"Train/Test size → {X_train.shape[0]} / {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cef7e7-9fbe-4de7-b5f6-bcc12167b332",
   "metadata": {},
   "source": [
    "### 5. Impute Missing Values\n",
    "We fit a `SimpleImputer` on the training features and transform both train and test sets, preserving the original row indices so that any later boolean‐masking stays aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0659509b-6e51-424b-b58c-a6d080c90502",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "# Fit on training, transform both sets, and preserve indices\n",
    "X_train_imp = pd.DataFrame(\n",
    "    imputer.fit_transform(X_train),\n",
    "    columns=feature_cols,\n",
    "    index=X_train.index           # <— preserve original train indices\n",
    ")\n",
    "X_test_imp = pd.DataFrame(\n",
    "    imputer.transform(X_test),\n",
    "    columns=feature_cols,\n",
    "    index=X_test.index            # <— preserve original test indices\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39bcde2-69e0-41b1-add0-7e7496e1edf2",
   "metadata": {},
   "source": [
    "## 6. Initialize classifiers & parameter grids  \n",
    "- Use CPU implementations by default  \n",
    "- If `USE_GPU=True`, enable GPU training for XGBoost and LightGBM  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099fcc5-ea9a-4adc-b5c0-08b4d6124079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 6 – Define classifiers with safe GPU logic\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# 1) Logistic Regression (CPU only)\n",
    "lr_clf = LogisticRegression(class_weight=\"balanced\", max_iter=2000, random_state=42)\n",
    "\n",
    "# 2) Random Forest (CPU only)\n",
    "rf_clf = RandomForestClassifier(class_weight=\"balanced\", n_jobs=1, random_state=42)\n",
    "\n",
    "# 3) XGBoost Classifier\n",
    "xgb_params = dict(tree_method=\"hist\", eval_metric=\"logloss\", random_state=42)\n",
    "xgb_params[\"device\"] = \"cuda\" if USE_GPU else \"cpu\"\n",
    "xgb_clf = xgb.XGBClassifier(**xgb_params)\n",
    "\n",
    "# 4) LightGBM Classifier\n",
    "lgb_params = dict(n_estimators=100, max_depth=7, learning_rate=0.05, random_state=42)\n",
    "if USE_GPU:\n",
    "    lgb_params.update(device=\"gpu\", gpu_platform_id=0, gpu_device_id=0)\n",
    "lgb_clf = LGBMClassifier(**lgb_params)\n",
    "\n",
    "# 5) Hyperparameter grids\n",
    "param_lr  = {\"C\": [0.1, 1]}\n",
    "param_rf  = {\"n_estimators\": [50, 100], \"max_depth\": [5, 10]}\n",
    "param_xgb = {\"n_estimators\": [100, 200], \"max_depth\": [3, 5], \"learning_rate\": [0.05, 0.1]}\n",
    "param_lgb = {\"n_estimators\": [100, 200], \"max_depth\": [5, 7], \"learning_rate\": [0.05, 0.1]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23edb86f-5990-4c04-85db-aaebdb4a730a",
   "metadata": {},
   "source": [
    "### 7. Fast Hyperparameter Tuning (full 75% training data, 2-fold CV, 2 candidates each)\n",
    "\n",
    "We’ll perform hyperparameter search on each model using the *entire* 75% training split (no subsampling), 2-fold time-series CV, and explore only 2 parameter combinations per model to keep the run-time reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ddcc8a-f00b-4fa1-a505-9ddd51ff8df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "from joblib import parallel_backend\n",
    "\n",
    "def tune_fast(model, params, X, y, name, max_iter=3, frac=0.3):\n",
    "    print(f\"\\n▶ Starting tuning for {name} at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    start = datetime.now()\n",
    "\n",
    "    # subsample for RandomForest only\n",
    "    if isinstance(model, RandomForestClassifier):\n",
    "        n = int(len(X) * frac)\n",
    "        idx = np.random.RandomState(42).choice(len(X), size=n, replace=False)\n",
    "        Xt, yt = X.iloc[idx], y.iloc[idx]\n",
    "    else:\n",
    "        Xt, yt = X, y\n",
    "\n",
    "    # choose Grid vs Random\n",
    "    total = len(list(ParameterGrid(params)))\n",
    "    if total <= max_iter:\n",
    "        search = GridSearchCV(\n",
    "            model, params,\n",
    "            cv=tscv,\n",
    "            scoring=\"accuracy\",\n",
    "            n_jobs=1,\n",
    "            verbose=2\n",
    "        )\n",
    "    else:\n",
    "        search = RandomizedSearchCV(\n",
    "            model, params,\n",
    "            n_iter=max_iter,\n",
    "            cv=tscv,\n",
    "            scoring=\"accuracy\",\n",
    "            n_jobs=1,\n",
    "            random_state=42,\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "    # fit using threading to avoid ResourceTracker errors\n",
    "    with parallel_backend('threading'):\n",
    "        search.fit(Xt, yt)\n",
    "\n",
    "    # report\n",
    "    elapsed = (datetime.now() - start).total_seconds()\n",
    "    print(f\"✔ Finished {name} in {elapsed:.1f}s\")\n",
    "    print(f\"{name} best params: {search.best_params_}\")\n",
    "    print(f\"{name} CV acc: {search.best_score_:.4f}\\n\")\n",
    "    return search.best_estimator_\n",
    "\n",
    "best_lr  = tune_fast(lr_clf,  param_lr,  X_train_imp, y_up_train, \"Logistic\")\n",
    "best_rf  = tune_fast(rf_clf,  param_rf,  X_train_imp, y_up_train, \"RandomForest\")\n",
    "best_xgb = tune_fast(xgb_clf, param_xgb, X_train_imp, y_up_train, \"XGBoost (GPU)\")\n",
    "best_lgb = tune_fast(lgb_clf, param_lgb, X_train_imp, y_up_train, \"LightGBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d19ba-d1ef-49d9-9def-3737198861d2",
   "metadata": {},
   "source": [
    "### 8. Build Prefit Stacking Ensemble\n",
    "Combine the four tuned estimators into a `StackingClassifier`, using a logistic meta‐learner on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0182b4-f5df-4988-82eb-35ca2f1a9b79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(f\"[{datetime.now().strftime('%H:%M:%S')}] Building stacking ensemble\")\n",
    "stack_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        (\"lr\",  best_lr),\n",
    "        (\"rf\",  best_rf),\n",
    "        (\"xgb\", best_xgb),\n",
    "        (\"lgb\", best_lgb),\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=2_000),\n",
    "    cv=\"prefit\",  # base models already trained\n",
    "    n_jobs=1\n",
    ")\n",
    "stack_clf.fit(X_train_imp, y_up_train)\n",
    "print(\"✅ Stacking complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd207003-5086-4ff5-af1d-e5fe4402073b",
   "metadata": {},
   "source": [
    "### 9. Evaluate on Test Set\n",
    "For each model, predict on the hold‐out set, print accuracy & classification report, and plot confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbeac9b-78ef-484f-8a3d-098055bcd1a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, mdl in [\n",
    "    (\"Logistic\", best_lr),\n",
    "    (\"RandomForest\", best_rf),\n",
    "    (\"XGBoost\", best_xgb),\n",
    "    (\"LightGBM\", best_lgb),\n",
    "    (\"Stacking\", stack_clf)\n",
    "]:\n",
    "    y_pred = mdl.predict(X_test_imp)\n",
    "    acc    = accuracy_score(y_up_test, y_pred)\n",
    "    print(f\"\\n► {name} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_up_test, y_pred))\n",
    "    sns.heatmap(confusion_matrix(y_up_test, y_pred), annot=True, fmt=\"d\")\n",
    "    plt.title(name); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6d277-f64f-40b8-9ccd-f9157f59b895",
   "metadata": {},
   "source": [
    "### 10. Precision–Recall Threshold Optimization for Stacker\n",
    "Find the probability cutoff that maximizes F1 on the test set and show its new accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891006c3-7ad2-4a30-82e1-92468cadbaa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "probs = stack_clf.predict_proba(X_test_imp)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(y_up_test, probs)\n",
    "f1 = 2 * prec * rec / (prec + rec)\n",
    "opt_thresh = thr[np.nanargmax(f1)]\n",
    "print(f\"Optimal threshold: {opt_thresh:.3f}\")\n",
    "\n",
    "y_opt = (probs >= opt_thresh).astype(int)\n",
    "print(\"Stacking@opt Threshold Accuracy:\", accuracy_score(y_up_test, y_opt))\n",
    "print(classification_report(y_up_test, y_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50fa19f",
   "metadata": {},
   "source": [
    "### 11. Tune & Evaluate Regressors for High/Low Prediction\n",
    "1. Drop rows where target is NaN.\n",
    "2. Remove zero‐variance features.\n",
    "3. Define RF, LGBM, XGB regressors (GPU).\n",
    "4. Run RandomizedSearchCV to minimize RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa5e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 – Define regressors with safe GPU logic\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# 1) Random Forest Regressor (CPU only)\n",
    "rf_regr = RandomForestRegressor(n_jobs=1, random_state=42)\n",
    "\n",
    "# 2) LightGBM Regressor\n",
    "lgb_regr_params = dict(\n",
    "    n_estimators=100,\n",
    "    max_depth=7,\n",
    "    min_child_samples=20,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "if USE_GPU:\n",
    "    lgb_regr_params.update(device=\"gpu\", gpu_platform_id=0, gpu_device_id=0)\n",
    "lgb_regr = LGBMRegressor(**lgb_regr_params)\n",
    "\n",
    "# 3) XGBoost Regressor\n",
    "xgb_regr_params = dict(\n",
    "    tree_method=\"hist\",\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    random_state=42\n",
    ")\n",
    "xgb_regr_params[\"device\"] = \"cuda\" if USE_GPU else \"cpu\"\n",
    "xgb_regr = xgb.XGBRegressor(**xgb_regr_params)\n",
    "\n",
    "# 4) You can now safely tune these regressors downstream…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ad731",
   "metadata": {},
   "source": [
    "### 12. Visualize Regressor Performance\n",
    "For each best regressor, compute test RMSE and scatter true vs predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac51910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def eval_reg(name, model, X_te, y_te):\n",
    "    yp = model.predict(X_te)\n",
    "    rmse = np.sqrt(mean_squared_error(y_te, yp))\n",
    "    print(f\"{name} — RMSE: {rmse:.2f}\")\n",
    "    plt.scatter(y_te, yp, alpha=0.3)\n",
    "    plt.plot([y_te.min(), y_te.max()],[y_te.min(),y_te.max()],'r--')\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "\n",
    "for nm, mdl, ytest in [\n",
    "    (\"LGB High\", best_regr[\"LGB High\"], y_high_test),\n",
    "    (\"XGB High\", best_regr[\"XGB High\"], y_high_test),\n",
    "    (\"RF High\",  best_regr[\"RF High\"],  y_high_test),\n",
    "    (\"LGB Low\",  best_regr[\"LGB Low\"],  y_low_test),\n",
    "    (\"XGB Low\",  best_regr[\"XGB Low\"],  y_low_test),\n",
    "    (\"RF Low\",   best_regr[\"RF Low\"],   y_low_test),\n",
    "]:\n",
    "    eval_reg(nm, mdl, X_test_imp[keep_cols], ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c391853e",
   "metadata": {},
   "source": [
    "### Full‐Data CV, Select & Retrain Best Model, Then Save\n",
    "1. Load the full processed dataset.  \n",
    "2. Perform time‐series CV on each candidate to compare mean accuracies.  \n",
    "3. Retrain the winning model on all data.  \n",
    "4. Write the model artifact under the parameterized `MODEL_DIR`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c3033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load full processed data\n",
    "df_full = pd.read_csv(\n",
    "    os.path.join(DATA_PROCESSED, \"historical_proc.csv\"),\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "df_full.sort_values(\"date\", inplace=True)\n",
    "\n",
    "Xf = pd.DataFrame(imputer.transform(df_full[feature_cols]), columns=feature_cols)\n",
    "yf = df_full[\"target_up\"]\n",
    "\n",
    "# 2) Full‐data time‐series CV\n",
    "tscv_full = TimeSeriesSplit(n_splits=3)\n",
    "candidates = {\n",
    "    \"Logistic\":     best_lr,\n",
    "    \"RandomForest\": best_rf,\n",
    "    \"XGBoost\":      best_xgb,\n",
    "    \"LightGBM\":     best_lgb,\n",
    "    \"Stacking\":     stack_clf\n",
    "}\n",
    "\n",
    "scores = {}\n",
    "for name, model in candidates.items():\n",
    "    sc = cross_val_score(model, Xf, yf, cv=tscv_full,\n",
    "                         scoring=\"accuracy\", n_jobs=1)\n",
    "    scores[name] = sc.mean()\n",
    "    print(f\"{name} full‐CV: {sc.mean():.4f} ± {sc.std():.4f}\")\n",
    "\n",
    "# 3) Select best and retrain on all data\n",
    "best_name   = max(scores, key=scores.get)\n",
    "final_model = candidates[best_name]\n",
    "print(f\"\\n▶ Best on full‐CV: {best_name}\")\n",
    "final_model.fit(Xf, yf)\n",
    "\n",
    "# 4) Save the final model artifact\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "out_path = os.path.join(MODEL_DIR, f\"final_{best_name.lower()}.pkl\")\n",
    "joblib.dump(final_model, out_path)\n",
    "print(f\"✅ Saved {os.path.basename(out_path)} to {MODEL_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
