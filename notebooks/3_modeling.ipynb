{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7d9de4-75fc-45cf-9b6a-592745a76df0",
   "metadata": {},
   "source": [
    "# 3. Modeling & Final Training\n",
    "\n",
    "_In this notebook we:_  \n",
    "1. Load the fully processed dataset from `data/processed/historical_proc.csv`.  \n",
    "2. Verify that the target column (`target_up`) exists.  \n",
    "3. Impute any remaining missing values.  \n",
    "4. Define and tune our classifiers and regressors.  \n",
    "5. Use TimeSeriesSplit for honest CV.  \n",
    "6. Select the best classifier model.  \n",
    "7. Retrain that classifier on 100% of the data.  \n",
    "8. Save the final model for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a92773",
   "metadata": {},
   "source": [
    "### 1. Parameters & Papermill Setup\n",
    "  1. Declare all variables you’ll define later so VS Code/Pylance stops complaining about `… is not defined`.  \n",
    "  2. Set `DATA_PROCESSED` and `MODEL_DIR` for Papermill to override.  \n",
    "  3. Default `USE_GPU=False` so on a CPU-only CI runner no GPU code paths are activated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a05e415",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell 1 – Parameters for Papermill & runtime\n",
    "\n",
    "DATA_PROCESSED = \"data/processed\"   # Papermill will replace if passed -p\n",
    "MODEL_DIR      = \"src/models\"       # Papermill will replace if passed -p\n",
    "\n",
    "# By default off in CI; set CI_USE_GPU=true in your workflow if you have a GPU runner\n",
    "USE_GPU = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c8792b-b3bd-47bf-b551-33aa5c82b407",
   "metadata": {},
   "source": [
    "### 2. Imports, Styling & GPU Flag\n",
    "\n",
    "- **multiprocessing.set_start_method('fork')** to avoid the `resource_tracker` warnings when using `n_jobs>1`.  \n",
    "- Suppress deprecation/device warnings.  \n",
    "- `%load_ext autotime` gives you per-cell timing.  \n",
    "- Re-evaluate `USE_GPU` from the `CI_USE_GPU` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0479201-b730-4db0-90eb-adbc74de367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 – Imports & GPU flag\n",
    "\n",
    "import multiprocessing as mp\n",
    "mp.set_start_method('fork', force=True)\n",
    "\n",
    "import os, warnings\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV, GridSearchCV, TimeSeriesSplit,\n",
    "    ParameterGrid, cross_val_score\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, StackingClassifier,\n",
    "    RandomForestRegressor\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    mean_squared_error, precision_recall_curve\n",
    ")\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "import joblib\n",
    "\n",
    "# Suppress scikit-learn deprecation / unused-param warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*not used.*\")\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "\n",
    "# Autotime: prints each cell’s run time\n",
    "%load_ext autotime\n",
    "\n",
    "# Re-evaluate USE_GPU if the CI runner sets CI_USE_GPU=true\n",
    "USE_GPU = os.environ.get(\"CI_USE_GPU\", \"false\").lower() in (\"1\",\"true\",\"yes\")\n",
    "print(f\"→ USE_GPU = {USE_GPU}\")\n",
    "\n",
    "# ── Pre-declare so Pylance knows they exist ───────────────────────\n",
    "best_lr = best_rf = best_xgb = best_lgb = None\n",
    "stack_clf = None\n",
    "feature_cols = []\n",
    "imputer = None\n",
    "X_train_imp = None\n",
    "X_test_imp = None\n",
    "keep_cols = []\n",
    "best_regr = {}\n",
    "# ────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b9a3eb-3f09-4c99-a1c6-c9caba8a7322",
   "metadata": {},
   "source": [
    "### 3. Load Features & Build Targets\n",
    "\n",
    "- Read in the processed features CSV.  \n",
    "- Sort by date, shift up to create “next-day” columns.  \n",
    "- Define three targets:  \n",
    "  - `target_up`: binary up/down tomorrow  \n",
    "  - `target_high`, `target_low`: regression targets.  \n",
    "- Drop the final row (it has no “next-day” values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d7a1c-eeae-4d6b-a735-435387e3a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 – Load data and build targets\n",
    "\n",
    "print(\"DEBUG: data/processed contains:\", os.listdir(DATA_PROCESSED))\n",
    "\n",
    "df = pd.read_csv(\n",
    "    os.path.join(DATA_PROCESSED, \"historical_proc.csv\"),\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "df.sort_values(\"date\", inplace=True)\n",
    "\n",
    "# Create next-day columns\n",
    "df[\"close_next\"] = df[\"close\"].shift(-1)\n",
    "df[\"high_next\"]  = df[\"high\"].shift(-1)\n",
    "df[\"low_next\"]   = df[\"low\"].shift(-1)\n",
    "\n",
    "# Define targets\n",
    "df[\"target_up\"]   = (df[\"close_next\"] > df[\"close\"]).astype(int)\n",
    "df[\"target_high\"] = df[\"high_next\"]\n",
    "df[\"target_low\"]  = df[\"low_next\"]\n",
    "\n",
    "# Drop the last row (NaNs in targets)\n",
    "df.dropna(subset=[\"target_up\",\"target_high\",\"target_low\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43dbf95-228a-4354-a4f9-62aecc469d80",
   "metadata": {},
   "source": [
    "### 4. Train / Test Split\n",
    "\n",
    "- Select feature columns (`_std`, `_mm`, `PC*`, and date parts).  \n",
    "- 75% of data for training, 25% for testing, preserving the time order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab42e11-52fe-4565-bfac-20e339fcb1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 – Train/Test split\n",
    "\n",
    "feature_cols = [\n",
    "    c for c in df.columns\n",
    "    if c.endswith((\"_std\",\"_mm\"))    # scaled numeric\n",
    "    or c.startswith(\"PC\")            # PCA components\n",
    "    or c in [\"year\",\"month\",\"day\",\"weekday\"]\n",
    "]\n",
    "\n",
    "X      = df[feature_cols]\n",
    "y_up   = df[\"target_up\"]\n",
    "y_high = df[\"target_high\"]\n",
    "y_low  = df[\"target_low\"]\n",
    "\n",
    "split = int(len(df) * 0.75)\n",
    "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "y_up_train, y_up_test       = y_up.iloc[:split], y_up.iloc[split:]\n",
    "y_high_train, y_high_test   = y_high.iloc[:split], y_high.iloc[split:]\n",
    "y_low_train,  y_low_test    = y_low.iloc[:split],  y_low.iloc[split:]\n",
    "\n",
    "print(f\"Train/Test size → {X_train.shape[0]} / {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cef7e7-9fbe-4de7-b5f6-bcc12167b332",
   "metadata": {},
   "source": [
    "### 5. Impute Missing Values\n",
    "\n",
    "- Use a `SimpleImputer(strategy=\"mean\")`.  \n",
    "- **Preserve** original DataFrame indices so later boolean masking stays aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0659509b-6e51-424b-b58c-a6d080c90502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 – Impute missing values\n",
    "\n",
    "%%time\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "X_train_imp = pd.DataFrame(\n",
    "    imputer.fit_transform(X_train),\n",
    "    columns=feature_cols,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_imp  = pd.DataFrame(\n",
    "    imputer.transform(X_test),\n",
    "    columns=feature_cols,\n",
    "    index=X_test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39bcde2-69e0-41b1-add0-7e7496e1edf2",
   "metadata": {},
   "source": [
    "### 6. Initialize classifiers & parameter grids\n",
    "\n",
    "Here we:\n",
    "\n",
    "1. Instantiate each model once (LR and RF always on CPU).  \n",
    "2. For XGBoost, if `USE_GPU` is true we use `tree_method=\"gpu_hist\"` + `gpu_id=0`; otherwise we fall back to plain `\"hist\"` on CPU.  \n",
    "3. For LightGBM, we only add the `device=\"gpu\", gpu_platform_id, gpu_device_id` keys when `USE_GPU`.  \n",
    "4. Finally we define small hyperparameter grids for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099fcc5-ea9a-4adc-b5c0-08b4d6124079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 6 – Initialize classifiers & parameter grids\n",
    "\n",
    "# 0) Make sure we have our 3-fold TimeSeriesSplit object ready\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# 1) Logistic Regression (CPU only)\n",
    "lr_clf = LogisticRegression(\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=2000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2) Random Forest (CPU only)\n",
    "rf_clf = RandomForestClassifier(\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3) XGBoost Classifier\n",
    "#   - On GPU: use gpu_hist + gpu_id\n",
    "#   - On CPU: use hist\n",
    "xgb_params = {\n",
    "    \"tree_method\": \"gpu_hist\" if USE_GPU else \"hist\",\n",
    "    \"eval_metric\": \"logloss\",\n",
    "    \"random_state\": 42\n",
    "}\n",
    "if USE_GPU:\n",
    "    xgb_params[\"gpu_id\"] = 0\n",
    "xgb_clf = xgb.XGBClassifier(**xgb_params)\n",
    "\n",
    "# 4) LightGBM Classifier\n",
    "#   - Only pass GPU args if USE_GPU\n",
    "lgb_params = dict(\n",
    "    n_estimators=100,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "if USE_GPU:\n",
    "    lgb_params.update(\n",
    "        device=\"gpu\",\n",
    "        gpu_platform_id=0,\n",
    "        gpu_device_id=0\n",
    "    )\n",
    "lgb_clf = LGBMClassifier(**lgb_params)\n",
    "\n",
    "# 5) Hyperparameter grids for tuning\n",
    "param_lr  = {\"C\": [0.1, 1]}\n",
    "param_rf  = {\"n_estimators\": [50, 100], \"max_depth\": [5, 10]}\n",
    "param_xgb = {\"n_estimators\": [100, 200], \"max_depth\": [3, 5], \"learning_rate\": [0.05, 0.1]}\n",
    "param_lgb = {\"n_estimators\": [100, 200], \"max_depth\": [5, 7], \"learning_rate\": [0.05, 0.1]}\n",
    "\n",
    "print(\"✅ Cell 6 complete: classifiers & grids defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23edb86f-5990-4c04-85db-aaebdb4a730a",
   "metadata": {},
   "source": [
    "### 7. Fast Hyperparameter Tuning\n",
    "\n",
    "- A helper `tune_fast` that:  \n",
    "  1. Sub-samples RF on a fraction of the data  \n",
    "  2. Chooses `GridSearchCV` vs. `RandomizedSearchCV`  \n",
    "  3. Uses a thread pool (`parallel_backend('threading')`) to avoid resource-tracker issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ddcc8a-f00b-4fa1-a505-9ddd51ff8df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "from joblib import parallel_backend\n",
    "\n",
    "def tune_fast(model, params, X, y, name, max_iter=3, frac=0.3):\n",
    "    print(f\"\\n▶ Starting tuning for {name} at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    start = datetime.now()\n",
    "\n",
    "    # subsample for RandomForest only\n",
    "    if isinstance(model, RandomForestClassifier):\n",
    "        n = int(len(X) * frac)\n",
    "        idx = np.random.RandomState(42).choice(len(X), size=n, replace=False)\n",
    "        Xt, yt = X.iloc[idx], y.iloc[idx]\n",
    "    else:\n",
    "        Xt, yt = X, y\n",
    "\n",
    "    # choose Grid vs Random\n",
    "    total = len(list(ParameterGrid(params)))\n",
    "    if total <= max_iter:\n",
    "        search = GridSearchCV(\n",
    "            model, params,\n",
    "            cv=tscv,\n",
    "            scoring=\"accuracy\",\n",
    "            n_jobs=1,\n",
    "            verbose=2\n",
    "        )\n",
    "    else:\n",
    "        search = RandomizedSearchCV(\n",
    "            model, params,\n",
    "            n_iter=max_iter,\n",
    "            cv=tscv,\n",
    "            scoring=\"accuracy\",\n",
    "            n_jobs=1,\n",
    "            random_state=42,\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "    # fit using threading to avoid ResourceTracker errors\n",
    "    with parallel_backend('threading'):\n",
    "        search.fit(Xt, yt)\n",
    "\n",
    "    # report\n",
    "    elapsed = (datetime.now() - start).total_seconds()\n",
    "    print(f\"✔ Finished {name} in {elapsed:.1f}s\")\n",
    "    print(f\"{name} best params: {search.best_params_}\")\n",
    "    print(f\"{name} CV acc: {search.best_score_:.4f}\\n\")\n",
    "    return search.best_estimator_\n",
    "\n",
    "best_lr  = tune_fast(lr_clf,  param_lr,  X_train_imp, y_up_train, \"Logistic\")\n",
    "best_rf  = tune_fast(rf_clf,  param_rf,  X_train_imp, y_up_train, \"RandomForest\")\n",
    "best_xgb = tune_fast(xgb_clf, param_xgb, X_train_imp, y_up_train, \"XGBoost (GPU)\")\n",
    "best_lgb = tune_fast(lgb_clf, param_lgb, X_train_imp, y_up_train, \"LightGBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d19ba-d1ef-49d9-9def-3737198861d2",
   "metadata": {},
   "source": [
    "### 8. Build & Train Prefitted Stacking\n",
    "\n",
    "- Stack the four tuned CPU/GPU-safe base models.  \n",
    "- Use a `LogisticRegression` meta-learner.  \n",
    "- `cv=\"prefit\"` since base models are already trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0182b4-f5df-4988-82eb-35ca2f1a9b79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 8 – Prefitted stacking ensemble\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"[{datetime.now():%H:%M:%S}] Building stacking ensemble\")\n",
    "stack_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        (\"lr\",  best_lr),\n",
    "        (\"rf\",  best_rf),\n",
    "        (\"xgb\", best_xgb),\n",
    "        (\"lgb\", best_lgb),\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=2000),\n",
    "    cv=\"prefit\",\n",
    "    n_jobs=1\n",
    ")\n",
    "stack_clf.fit(X_train_imp, y_up_train)\n",
    "print(\"✅ Stacking complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd207003-5086-4ff5-af1d-e5fe4402073b",
   "metadata": {},
   "source": [
    "### 9. Evaluate on Test Set\n",
    "\n",
    "- For each model, compute accuracy, print a classification report, and show a confusion matrix heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbeac9b-78ef-484f-8a3d-098055bcd1a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 9 – Test-set evaluation\n",
    "\n",
    "for name, mdl in [\n",
    "    (\"Logistic\", best_lr),\n",
    "    (\"RandomForest\", best_rf),\n",
    "    (\"XGBoost\", best_xgb),\n",
    "    (\"LightGBM\", best_lgb),\n",
    "    (\"Stacking\", stack_clf)\n",
    "]:\n",
    "    y_pred = mdl.predict(X_test_imp)\n",
    "    acc    = accuracy_score(y_up_test, y_pred)\n",
    "    print(f\"\\n► {name} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_up_test, y_pred))\n",
    "    sns.heatmap(confusion_matrix(y_up_test, y_pred), annot=True, fmt=\"d\")\n",
    "    plt.title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6d277-f64f-40b8-9ccd-f9157f59b895",
   "metadata": {},
   "source": [
    "### 10. Precision–Recall Threshold Optimization\n",
    "\n",
    "- Compute the precision–recall curve on the stacking model’s probabilities.  \n",
    "- Pick the threshold that maximizes F₁, then re-evaluate at that threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891006c3-7ad2-4a30-82e1-92468cadbaa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 10 – PR-curve threshold optimization\n",
    "\n",
    "probs = stack_clf.predict_proba(X_test_imp)[:,1]\n",
    "prec, rec, thr = precision_recall_curve(y_up_test, probs)\n",
    "\n",
    "f1 = 2 * prec * rec / (prec + rec)\n",
    "opt_thresh = thr[np.nanargmax(f1)]\n",
    "print(f\"Optimal threshold: {opt_thresh:.3f}\")\n",
    "\n",
    "y_opt = (probs >= opt_thresh).astype(int)\n",
    "print(\"Stacking@opt Threshold Accuracy:\", accuracy_score(y_up_test, y_opt))\n",
    "print(classification_report(y_up_test, y_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50fa19f",
   "metadata": {},
   "source": [
    "### 11. Define & (Optionally) Tune Regressors\n",
    "\n",
    "- We’ll predict tomorrow’s high/low with RF/LGBM/XGB regressors.  \n",
    "- Same `USE_GPU` guard ensures no OpenCL errors in CI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa5e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Cell 11 – Optional: Tune Regressors for High/Low Price Prediction\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection   import RandomizedSearchCV\n",
    "from sklearn.ensemble          import RandomForestRegressor\n",
    "from lightgbm                  import LGBMRegressor\n",
    "import xgboost                 as xgb\n",
    "from joblib                    import parallel_backend\n",
    "\n",
    "# 1) Subset training data, dropping NaNs in each target\n",
    "mask_h = ~y_high_train.isna()\n",
    "mask_l = ~y_low_train.isna()\n",
    "Xh, yh = X_train_imp.loc[mask_h], y_high_train.loc[mask_h]\n",
    "Xl, yl = X_train_imp.loc[mask_l], y_low_train.loc[mask_l]\n",
    "\n",
    "# 2) Remove zero‐variance features (they carry no information)\n",
    "vt = VarianceThreshold(threshold=0.0)\n",
    "vt.fit(X_train_imp)\n",
    "keep_mask = vt.get_support()\n",
    "keep_cols = [c for c, keep in zip(feature_cols, keep_mask) if keep]\n",
    "Xh = Xh[keep_cols]\n",
    "Xl = Xl[keep_cols]\n",
    "\n",
    "# 3) Define regressors with GPU‐safe flags\n",
    "rf_regr = RandomForestRegressor(n_jobs=1, random_state=42)\n",
    "\n",
    "lgb_params = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 7,\n",
    "    \"min_child_samples\": 20,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "if USE_GPU:\n",
    "    # only enable LightGBM GPU when CI_USE_GPU is true\n",
    "    lgb_params.update(device=\"gpu\", gpu_platform_id=0, gpu_device_id=0)\n",
    "lgb_regr = LGBMRegressor(**lgb_params)\n",
    "\n",
    "xgb_params = {\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"random_state\": 42,\n",
    "    \"device\": \"cuda\" if USE_GPU else \"cpu\"\n",
    "}\n",
    "xgb_regr = xgb.XGBRegressor(**xgb_params)\n",
    "\n",
    "# 4) Hyperparameter grids for each regressor\n",
    "param_rf   = {\"n_estimators\": [50, 100],         \"max_depth\": [5, 10]}\n",
    "param_lgb  = {\"n_estimators\": [100, 200],        \"max_depth\": [5, 7],  \"learning_rate\": [0.05, 0.1]}\n",
    "param_xgb  = {\"n_estimators\": [100, 200],        \"max_depth\": [3, 5],  \"learning_rate\": [0.05, 0.1]}\n",
    "\n",
    "models = {\n",
    "    \"RF High\":  (rf_regr,  param_rf,  Xh, yh),\n",
    "    \"LGB High\": (lgb_regr, param_lgb, Xh, yh),\n",
    "    \"XGB High\": (xgb_regr, param_xgb, Xh, yh),\n",
    "    \"RF Low\":   (rf_regr,  param_rf,  Xl, yl),\n",
    "    \"LGB Low\":  (lgb_regr, param_lgb, Xl, yl),\n",
    "    \"XGB Low\":  (xgb_regr, param_xgb, Xl, yl),\n",
    "}\n",
    "\n",
    "best_regr = {}\n",
    "for name, (mdl, prm, Xtr, ytr) in models.items():\n",
    "    print(f\"\\n▶ Tuning {name}\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=mdl,\n",
    "        param_distributions=prm,\n",
    "        n_iter=3,\n",
    "        cv=tscv,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        random_state=42,\n",
    "        verbose=2,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    # use threading backend to avoid ResourceTracker errors\n",
    "    with parallel_backend('threading'):\n",
    "        search.fit(Xtr, ytr)\n",
    "    best_regr[name] = search.best_estimator_\n",
    "    print(f\"{name} best RMSE: {-search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ad731",
   "metadata": {},
   "source": [
    "### 12. Visualize Regressor Performance\n",
    "\n",
    "- Plot predicted vs. actual tomorrow’s high/low for each tuned regressor.  \n",
    "- Compute RMSE and overlay a 1:1 line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac51910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 – Evaluate regressors\n",
    "\n",
    "def eval_reg(name, model, X_te, y_te):\n",
    "    yp   = model.predict(X_te)\n",
    "    rmse = np.sqrt(mean_squared_error(y_te, yp))\n",
    "    print(f\"{name} — RMSE: {rmse:.2f}\")\n",
    "    plt.scatter(y_te, yp, alpha=0.3)\n",
    "    plt.plot([y_te.min(), y_te.max()],[y_te.min(), y_te.max()],'r--')\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "\n",
    "# Filter out any zero-variance columns used in training\n",
    "vt         = VarianceThreshold(threshold=0.0)\n",
    "vt.fit(X_train_imp)\n",
    "keep_cols  = [c for c, keep in zip(feature_cols, vt.get_support()) if keep]\n",
    "\n",
    "for nm, mdl, ytest in [\n",
    "    (\"LGB High\", best_regr[\"LGB High\"], y_high_test),\n",
    "    (\"XGB High\", best_regr[\"XGB High\"], y_high_test),\n",
    "    (\"RF High\",  best_regr[\"RF High\"],  y_high_test),\n",
    "    (\"LGB Low\",  best_regr[\"LGB Low\"],  y_low_test),\n",
    "    (\"XGB Low\",  best_regr[\"XGB Low\"],  y_low_test),\n",
    "    (\"RF Low\",   best_regr[\"RF Low\"],   y_low_test)\n",
    "]:\n",
    "    eval_reg(nm, mdl, X_test_imp[keep_cols], ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c391853e",
   "metadata": {},
   "source": [
    "### 13. Full-Data CV, Select & Retrain, Save\n",
    "\n",
    "1. Re-load the full dataset.  \n",
    "2. Time-series CV on **all** data to pick the overall best model.  \n",
    "3. Retrain that model on 100% of the data.  \n",
    "4. Save to `src/models/final_{model}.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c3033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 – Full-data CV & final save\n",
    "\n",
    "df_full = pd.read_csv(\n",
    "    os.path.join(DATA_PROCESSED, \"historical_proc.csv\"),\n",
    "    parse_dates=[\"date\"]\n",
    ")\n",
    "df_full.sort_values(\"date\", inplace=True)\n",
    "\n",
    "Xf = pd.DataFrame(imputer.transform(df_full[feature_cols]), columns=feature_cols)\n",
    "yf = df_full[\"target_up\"]\n",
    "\n",
    "tscv_full = TimeSeriesSplit(n_splits=3)\n",
    "candidates = {\n",
    "    \"Logistic\":     best_lr,\n",
    "    \"RandomForest\": best_rf,\n",
    "    \"XGBoost\":      best_xgb,\n",
    "    \"LightGBM\":     best_lgb,\n",
    "    \"Stacking\":     stack_clf\n",
    "}\n",
    "\n",
    "scores = {}\n",
    "for name, model in candidates.items():\n",
    "    sc = cross_val_score(\n",
    "        model, Xf, yf,\n",
    "        cv=tscv_full,\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=1\n",
    "    )\n",
    "    scores[name] = sc.mean()\n",
    "    print(f\"{name} full-CV: {sc.mean():.4f} ± {sc.std():.4f}\")\n",
    "\n",
    "best_name   = max(scores, key=scores.get)\n",
    "final_model = candidates[best_name]\n",
    "print(f\"\\n▶ Best on full-CV: {best_name}\")\n",
    "final_model.fit(Xf, yf)\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "out_path = os.path.join(MODEL_DIR, f\"final_{best_name.lower()}.pkl\")\n",
    "joblib.dump(final_model, out_path)\n",
    "print(f\"✅ Saved {os.path.basename(out_path)} to {MODEL_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
