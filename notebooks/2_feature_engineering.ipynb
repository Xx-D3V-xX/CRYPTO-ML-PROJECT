{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e5d854-0a1d-44ea-be4c-a858cf34673c",
   "metadata": {},
   "source": [
    "# Phase 2: Feature Engineering\n",
    "\n",
    "In this notebook we will:\n",
    "1. Load interim datasets.  \n",
    "2. Encode categorical variables.  \n",
    "3. Extract date-based features.  \n",
    "4. Scale numeric features (Standard & MinMax).  \n",
    "5. Visualize before/after scaling.  \n",
    "6. Feature Extraction: PCA & L1/L2.  \n",
    "7. Save processed datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4049077",
   "metadata": {},
   "source": [
    "### 1. Setup paths for interim & processed data\n",
    "Define `DATA_INTERIM` for cleaned inputs and `DATA_PROCESSED` for final feature‐engineered outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74976c82",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters cell for Papermill\n",
    "DATA_INTERIM   = \"data/interim\"\n",
    "DATA_PROCESSED = \"data/processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dddd99",
   "metadata": {},
   "source": [
    "### 2. Imports & Plot Configuration\n",
    "Load libs for scaling, PCA, imputation, and linear models; set plotting style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322461c9-85d3-4794-ba4d-462ebb4e7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition   import PCA\n",
    "from sklearn.impute          import SimpleImputer\n",
    "from sklearn.linear_model    import Lasso, Ridge\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ad3ef-b777-4918-b662-428c42450807",
   "metadata": {},
   "source": [
    "### 3. Load Interim CSVs\n",
    "Read the cleaned interim files into DataFrames for transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee72b2-dd1f-4169-94d4-9eb5634591cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "historical = pd.read_csv(f\"{DATA_INTERIM}/historical_clean.csv\", parse_dates=['date'])\n",
    "coins      = pd.read_csv(f\"{DATA_INTERIM}/coins_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520680de-b61f-4a39-af67-d243b9737625",
   "metadata": {},
   "source": [
    "### 4. One-Hot Encode Coin Attributes\n",
    "Convert `status` and `category` in `coins` into indicator columns, dropping the first level to avoid collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d8e23-cd09-44e0-bd6e-c33c8e6d106d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coins_ohe = pd.get_dummies(coins, columns=['status','category'], drop_first=True)\n",
    "# quick display to confirm\n",
    "display(coins_ohe.filter(like='status_').head())\n",
    "display(coins_ohe.filter(like='category_').head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b4a9f-6a73-44de-871f-4f25359f6543",
   "metadata": {},
   "source": [
    "### 5. Extract Date Features\n",
    "From the `date` column in `historical`, create numeric features: year, month, day, weekday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2520961-9bec-4d5b-9ed1-6c498dd14d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical['year']    = historical['date'].dt.year\n",
    "historical['month']   = historical['date'].dt.month\n",
    "historical['day']     = historical['date'].dt.day\n",
    "historical['weekday'] = historical['date'].dt.weekday\n",
    "display(historical[['date','year','month','day','weekday']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a876bafe",
   "metadata": {},
   "source": [
    "### 6. Scale Numeric Columns\n",
    "Apply both StandardScaler (z-score) and MinMaxScaler to key columns, appending `_std` and `_mm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2861d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols   = ['price','market_cap','volume_24h','percent_change_24h']\n",
    "scaler_std = StandardScaler()\n",
    "scaler_mm  = MinMaxScaler()\n",
    "\n",
    "std_vals = scaler_std.fit_transform(historical[num_cols])\n",
    "mm_vals  = scaler_mm.fit_transform(historical[num_cols])\n",
    "\n",
    "hist_std = pd.DataFrame(std_vals, columns=[f\"{c}_std\" for c in num_cols])\n",
    "hist_mm  = pd.DataFrame(mm_vals,  columns=[f\"{c}_mm\"  for c in num_cols])\n",
    "\n",
    "historical = pd.concat([historical, hist_std, hist_mm], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a9d5d-3878-469b-8f4d-6b85b50b28e5",
   "metadata": {},
   "source": [
    "### 7. Visualize Distributions of Original vs Scaled\n",
    "For each original and scaled column, sample up to 100 000 points, then plot histograms side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a00474-9983-41bc-9bee-7ad33d2fd4a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def subsample(s, n=100000):\n",
    "    return s.sample(n, random_state=42) if len(s)>n else s\n",
    "\n",
    "for orig in num_cols:\n",
    "    std_col = f\"{orig}_std\"\n",
    "    mm_col  = f\"{orig}_mm\"\n",
    "    for col in (orig, std_col, mm_col):\n",
    "        data = subsample(historical[col].dropna())\n",
    "        sns.histplot(data, bins=50).set_title(col)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0602ebe-18b4-473e-8c4b-91e9fc8c5b43",
   "metadata": {},
   "source": [
    "### 8. PCA Variance & Component Selection\n",
    "1. Impute any NaNs in scaled features.\n",
    "2. Fit PCA and plot cumulative explained variance.\n",
    "3. Determine how many components are needed for ≥90 % variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e46b7-7cd2-4c1f-a2d9-74383a3d1edd",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaled_cols = [c for c in historical.columns if c.endswith((\"_std\",\"_mm\"))]\n",
    "imputer     = SimpleImputer(strategy='mean')\n",
    "X_scaled    = imputer.fit_transform(historical[scaled_cols])\n",
    "\n",
    "pca       = PCA()\n",
    "pca.fit(X_scaled)\n",
    "cum_var   = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.plot(range(1,len(cum_var)+1), cum_var, marker='o')\n",
    "plt.axhline(0.90, color='r', linestyle='--', label='90% variance')\n",
    "plt.xlabel(\"PCA components\"); plt.ylabel(\"Cumulative variance\")\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "n_components_90 = np.searchsorted(cum_var, 0.90) + 1\n",
    "print(f\"Components for 90% variance: {n_components_90}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a28f06-f12d-47ea-843d-0952da488a78",
   "metadata": {},
   "source": [
    "### 9. Feature Selection via Lasso & Ridge\n",
    "Fit Lasso and Ridge to predict `price` from all scaled features; rank by absolute L1 coefficient to find top 10 drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215802a8-3bcb-4d2f-ac88-d27128cfa82b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_fs = X_scaled\n",
    "y_fs = historical['price'].values\n",
    "\n",
    "lasso = Lasso(alpha=0.1, random_state=42).fit(X_fs, y_fs)\n",
    "ridge = Ridge(alpha=1.0, random_state=42).fit(X_fs, y_fs)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature':    scaled_cols,\n",
    "    'coef_lasso': lasso.coef_,\n",
    "    'coef_ridge': ridge.coef_\n",
    "}).set_index('feature')\n",
    "\n",
    "coef_df['abs_lasso'] = coef_df['coef_lasso'].abs()\n",
    "top10 = coef_df.sort_values('abs_lasso', ascending=False).head(10)\n",
    "display(top10[['coef_lasso','coef_ridge']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9ddfd7-c0a5-4e4e-ad6d-b45beb2996d7",
   "metadata": {},
   "source": [
    "### 10. Append Selected PCA Components & Save\n",
    "1. Fit PCA with `n_components_90`.\n",
    "2. Concatenate those PCs to `historical`.\n",
    "3. Write out `historical_proc.csv` and `coins_ohe.csv` for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3adddd-cb00-4fae-b1aa-867942316d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) compute the final PCA\n",
    "pca_final = PCA(n_components=n_components_90)\n",
    "X_full_scaled = imputer.transform(historical[scaled_cols])\n",
    "pca_comps     = pca_final.fit_transform(X_full_scaled)\n",
    "pca_cols      = [f\"PC{i+1}\" for i in range(n_components_90)]\n",
    "df_pca        = pd.DataFrame(pca_comps, columns=pca_cols, index=historical.index)\n",
    "\n",
    "# 2) merge & save\n",
    "historical_final = pd.concat([historical, df_pca], axis=1)\n",
    "os.makedirs(DATA_PROCESSED, exist_ok=True)\n",
    "historical_final.to_csv(f\"{DATA_PROCESSED}/historical_proc.csv\", index=False)\n",
    "coins_ohe.to_csv(f\"{DATA_PROCESSED}/coins_ohe.csv\",       index=False)\n",
    "print(\"✅ Saved processed datasets.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
