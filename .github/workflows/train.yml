name: train

on:
  push:
    paths:
      - 'notebooks/**'
      - '.github/workflows/train.yml'

jobs:
  check_changes:
    runs-on: ubuntu-latest
    outputs:
      eda:    ${{ steps.filter.outputs.eda }}
      fe:     ${{ steps.filter.outputs.fe }}
      model:  ${{ steps.filter.outputs.model }}
      config: ${{ steps.filter.outputs.config }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Detect changed notebooks
        id: filter
        uses: dorny/paths-filter@v3
        with:
          filters: |
            eda:
              - 'notebooks/1_eda_preprocessing.ipynb'
            fe:
              - 'notebooks/2_feature_engineering.ipynb'
            model:
              - 'notebooks/3_modeling.ipynb'
            config:
              - '.github/workflows/train.yml'

  eda:
    needs: check_changes
    if: |
      needs.check_changes.outputs.eda   == 'true' ||
      needs.check_changes.outputs.config == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Install Kaggle CLI
        run: pip install kaggle

      - name: Configure Kaggle credentials
        run: |
          mkdir -p ~/.kaggle
          cat <<EOF > ~/.kaggle/kaggle.json
          {
            "username":"${{ secrets.KAGGLE_USERNAME }}",
            "key":"${{ secrets.KAGGLE_KEY }}"
          }
          EOF
          chmod 600 ~/.kaggle/kaggle.json

      - name: Download CoinMarketCap historical
        run: |
          kaggle datasets download -d bizzyvinci/coinmarketcap-historical-data -p data/raw/
          unzip -o data/raw/coinmarketcap-historical-data.zip -d data/raw/

      - name: Download Sentiment data (BTC/ETH/BNB/ADA)
        run: |
          kaggle datasets download -d gautamchettiar/historical-sentiment-data-btc-eth-bnb-ada -p data/raw/
          unzip -o data/raw/historical-sentiment-data-btc-eth-bnb-ada.zip -d data/raw/

      - name: Download Crypto news
        run: |
          kaggle datasets download -d oliviervha/crypto-news -p data/raw/
          unzip -o data/raw/crypto-news.zip -d data/raw/

      - name: Set up Python 3.13
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'

      - name: Install dependencies
        run: pip install papermill pandas seaborn scikit-learn missingno matplotlib ipykernel

      - name: Install Jupyter kernel
        run: |
          python -m ipykernel install \
            --sys-prefix \
            --name python3 \
            --display-name python3

      - name: Run Notebook 1:EDA & Preprocessing
        run: |
          mkdir -p reports data/interim
          papermill \
            notebooks/1_eda_preprocessing.ipynb \
            reports/1_eda_preprocessing_output.ipynb \
            -p DATA_RAW data/raw \
            -p DATA_INTERIM data/interim

      - name: Upload interim data
        uses: actions/upload-artifact@v4
        with:
          name: interim-data
          path: data/interim/

  fe:
    needs: [check_changes, eda]
    if: |
      needs.check_changes.outputs.fe     == 'true' ||
      needs.check_changes.outputs.eda    == 'true' ||
      needs.check_changes.outputs.config == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Download interim data
        uses: actions/download-artifact@v4
        with:
          name: interim-data
          path: data/interim/

      - name: Set up Python 3.13
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'

      - name: Install dependencies
        run: pip install papermill pandas seaborn scikit-learn matplotlib ipykernel

      - name: Install Jupyter kernel
        run: |
          python -m ipykernel install \
            --sys-prefix \
            --name python3 \
            --display-name python3

      - name: Run Notebook 2:Feature Engineering
        run: |
          mkdir -p reports data/processed
          papermill \
            notebooks/2_feature_engineering.ipynb \
            reports/2_feature_engineering_output.ipynb \
            -p DATA_INTERIM data/interim \
            -p DATA_PROCESSED data/processed

      - name: Upload processed data
        uses: actions/upload-artifact@v4
        with:
          name: processed-data
          path: data/processed/

  model:
    needs: [check_changes, fe]
    if: |
      needs.check_changes.outputs.model  == 'true' ||
      needs.check_changes.outputs.fe     == 'true' ||
      needs.check_changes.outputs.eda    == 'true' ||
      needs.check_changes.outputs.config == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Download processed data
        uses: actions/download-artifact@v4
        with:
          name: processed-data
          path: data/processed/

      - name: Set up Python 3.13
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'

      - name: Install dependencies
        run: pip install papermill pandas seaborn scikit-learn matplotlib xgboost lightgbm joblib ipykernel

      - name: Install Jupyter kernel
        run: |
          python -m ipykernel install \
            --sys-prefix \
            --name python3 \
            --display-name python3

      - name: Run Notebook 3:Modeling
        run: |
          mkdir -p reports src/models
          papermill \
            notebooks/3_modeling.ipynb \
            reports/3_modeling_output.ipynb \
            -p DATA_PROCESSED data/processed \
            -p MODEL_DIR src/models

      - name: Upload model artifacts
        uses: actions/upload-artifact@v4
        with:
          name: final-model
          path: src/models/