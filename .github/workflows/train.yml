name: train

on:
  push:
    paths:
      - 'notebooks/**'
      - '.github/workflows/train.yml'

jobs:
  check_changes:
    runs-on: ubuntu-latest
    outputs:
      eda:    ${{ steps.filter.outputs.eda }}
      fe:     ${{ steps.filter.outputs.fe }}
      model:  ${{ steps.filter.outputs.model }}
      config: ${{ steps.filter.outputs.config }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Detect changed notebooks & config
        id: filter
        uses: dorny/paths-filter@v3
        with:
          filters: |
            eda:
              - 'notebooks/1_eda_preprocessing.ipynb'
            fe:
              - 'notebooks/2_feature_engineering.ipynb'
            model:
              - 'notebooks/3_modeling.ipynb'
            config:
              - '.github/workflows/train.yml'

  eda:
    needs: check_changes
    if: |
      needs.check_changes.outputs.eda   == 'true' ||
      needs.check_changes.outputs.config == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Install Kaggle CLI
        run: pip install kaggle

      - name: Configure Kaggle credentials
        run: |
          mkdir -p ~/.kaggle
          cat <<EOF > ~/.kaggle/kaggle.json
          {
            "username": "${{ secrets.KAGGLE_USERNAME }}",
            "key": "${{ secrets.KAGGLE_KEY }}"
          }
          EOF
          chmod 600 ~/.kaggle/kaggle.json

      - name: Download CoinMarketCap historical
        run: |
          kaggle datasets download \
            -d bizzyvinci/coinmarketcap-historical-data \
            -p data/raw/
          unzip -o data/raw/coinmarketcap-historical-data.zip -d data/raw/

      - name: Download Sentiment data
        run: |
          kaggle datasets download \
            -d gautamchettiar/historical-sentiment-data-btc-eth-bnb-ada \
            -p data/raw/
          unzip -o data/raw/historical-sentiment-data-btc-eth-bnb-ada.zip -d data/raw/

      - name: Download Crypto news
        run: |
          kaggle datasets download \
            -d oliviervha/crypto-news \
            -p data/raw/
          unzip -o data/raw/crypto-news.zip -d data/raw/

      - name: Set up Python 3.13
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'

      - name: Install dependencies
        run: pip install papermill pandas seaborn scikit-learn missingno matplotlib

      - name: Run Notebook 1:EDA & Preprocessing
        run: |
          mkdir -p reports data/interim
          papermill \
            notebooks/1_eda_preprocessing.ipynb \
            reports/1_eda_preprocessing_output.ipynb \
            --log-output \
            -p DATA_RAW data/raw \
            -p DATA_INTERIM data/interim

      - name: Commit & push interim artifacts
        uses: Andro999b/push@v1.3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          directory: .
          message: "chore: update raw + interim data & EDA report"

  fe:
    needs: [check_changes, eda]
    if: |
      needs.check_changes.outputs.fe     == 'true' ||
      needs.check_changes.outputs.eda    == 'true' ||
      needs.check_changes.outputs.config == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Download interim data artifact
        uses: actions/download-artifact@v4
        with:
          name: interim-data
          path: data/interim/

      - name: Set up Python 3.13
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'

      - name: Install dependencies
        run: pip install papermill pandas seaborn scikit-learn matplotlib

      - name: Run Notebook 2:Feature Engineering
        run: |
          mkdir -p reports data/processed
          papermill \
            notebooks/2_feature_engineering.ipynb \
            reports/2_feature_engineering_output.ipynb \
            --log-output \
            -p DATA_INTERIM data/interim \
            -p DATA_PROCESSED data/processed

      - name: Commit & push processed artifacts
        uses: Andro999b/push@v1.3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          directory: .
          message: "chore: update processed data & FE report"

  model:
    needs: [check_changes, fe]
    if: |
      needs.check_changes.outputs.model  == 'true' ||
      needs.check_changes.outputs.fe     == 'true' ||
      needs.check_changes.outputs.eda    == 'true' ||
      needs.check_changes.outputs.config == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Download processed data artifact
        uses: actions/download-artifact@v4
        with:
          name: processed-data
          path: data/processed/

      - name: Set up Python 3.13
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'

      - name: Install dependencies
        run: pip install papermill pandas seaborn scikit-learn matplotlib xgboost lightgbm joblib

      - name: Run Notebook 3:Modeling
        run: |
          mkdir -p reports src/models
          papermill \
            notebooks/3_modeling.ipynb \
            reports/3_modeling_output.ipynb \
            --log-output \
            -p DATA_PROCESSED data/processed \
            -p MODEL_DIR src/models

      - name: Commit & push models & reports
        uses: Andro999b/push@v1.3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          directory: .
          message: "chore: update models & Modeling report"
